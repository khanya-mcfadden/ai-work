{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 09:09:57.696314: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-02 09:09:57.699039: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-02 09:09:57.736585: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 09:09:57.736628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 09:09:57.736652: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 09:09:57.745815: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-02 09:09:57.746638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 09:09:59.680057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import all tensorflow related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tfd\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# import other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as implt\n",
    "from IPython.display import clear_output as cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 150\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_NAME = \"CharacterRecognition-Model\"\n",
    "TRAIN_SIZE = BATCH_SIZE * 3000\n",
    "VALID_SIZE = BATCH_SIZE * 1500\n",
    "TEST_SIZE  = BATCH_SIZE * 300\n",
    "AUTOTUNE = tfd.AUTOTUNE\n",
    "\n",
    "# Training callbacks \n",
    "CALLBACKS = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath=MODEL_NAME + \".h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Paths \n",
    "train_csv_path = '/workspace/flasky/data/CSV/written_name_test.csv'\n",
    "valid_csv_path = '/workspace/flasky/data/CSV/written_name_validation.csv'\n",
    "test_csv_path = '/workspace/flasky/data/CSV/written_name_train.csv'\n",
    "train_image_dir = '/workspace/flasky/data/test_v2'\n",
    "valid_image_dir = '/workspace/flasky/data/validation_v2'\n",
    "test_image_dir = '/workspace/flasky/data/test_v2'\n",
    "\n",
    "# SetUp random seeds for numpy and TensorFlow\n",
    "np.random.seed(2569)\n",
    "tf.random.set_seed(2569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv files \n",
    "train_csv = pd.read_csv(train_csv_path)[:TRAIN_SIZE]\n",
    "test_csv = pd.read_csv(test_csv_path)[:TEST_SIZE]\n",
    "valid_csv = pd.read_csv(valid_csv_path)[:VALID_SIZE] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>IDENTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0001.jpg</td>\n",
       "      <td>KEVIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0002.jpg</td>\n",
       "      <td>CLOTAIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0003.jpg</td>\n",
       "      <td>LENA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0004.jpg</td>\n",
       "      <td>JULES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0005.jpg</td>\n",
       "      <td>CHERPIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FILENAME  IDENTITY\n",
       "0  TEST_0001.jpg     KEVIN\n",
       "1  TEST_0002.jpg  CLOTAIRE\n",
       "2  TEST_0003.jpg      LENA\n",
       "3  TEST_0004.jpg     JULES\n",
       "4  TEST_0005.jpg   CHERPIN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes(unique characters): 40\n"
     ]
    }
   ],
   "source": [
    "# get the train labels \n",
    "train_labels = [str(word) for word in train_csv[\"IDENTITY\"].to_numpy()]\n",
    "# extract all the unique characters\n",
    "unique_characters = set(char for word in train_labels for char in word)\n",
    "# define the number of classes (for labels) based on the number of unique characters\n",
    "n_classes = len(unique_characters)\n",
    "print(f\"Number of unique classes(unique characters): {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>IDENTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workspace/flasky/data/test_v2/TEST_0001.jpg</td>\n",
       "      <td>KEVIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workspace/flasky/data/test_v2/TEST_0002.jpg</td>\n",
       "      <td>CLOTAIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workspace/flasky/data/test_v2/TEST_0003.jpg</td>\n",
       "      <td>LENA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workspace/flasky/data/test_v2/TEST_0004.jpg</td>\n",
       "      <td>JULES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workspace/flasky/data/test_v2/TEST_0005.jpg</td>\n",
       "      <td>CHERPIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       FILENAME  IDENTITY\n",
       "0  /workspace/flasky/data/test_v2/TEST_0001.jpg     KEVIN\n",
       "1  /workspace/flasky/data/test_v2/TEST_0002.jpg  CLOTAIRE\n",
       "2  /workspace/flasky/data/test_v2/TEST_0003.jpg      LENA\n",
       "3  /workspace/flasky/data/test_v2/TEST_0004.jpg     JULES\n",
       "4  /workspace/flasky/data/test_v2/TEST_0005.jpg   CHERPIN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['FILENAME'] = [train_image_dir + f\"/{filename}\" for filename in train_csv['FILENAME']]\n",
    "valid_csv['FILENAME'] = [valid_image_dir + f\"/{filename}\" for filename in valid_csv['FILENAME']]\n",
    "test_csv['FILENAME']  = [test_image_dir + f\"/{filename}\" for filename in test_csv['FILENAME']]\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest label in the datase: 24\n"
     ]
    }
   ],
   "source": [
    "# get the maximum length that a label can have \n",
    "MAX_LABEL_LENGTH = max(map(len, train_labels))\n",
    "print(f\"The length of the longest label in the datase: {MAX_LABEL_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train labels \n",
    "train_labels = [str(word) for word in train_csv[\"IDENTITY\"].to_numpy()]\n",
    "\n",
    "# extract all the unique characters\n",
    "unique_characters = set(char for word in train_labels for char in word)\n",
    "# define the number of classes (for labels) based on the number of unique characters\n",
    "n_classes = len(unique_characters)\n",
    "\n",
    "# get the maximum length that a label can have \n",
    "MAX_LABEL_LENGTH = max(map(len, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_num = StringLookup(vocabulary=list(unique_characters), mask_token=None)\n",
    "num_to_char = StringLookup(vocabulary = char_to_num.get_vocabulary(), mask_token = None, invert = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    This function gets the image path and \n",
    "    reads the image usin TensorFlow, Then the image will be decoded and \n",
    "    will be converted to float data type. next resize and transpose will be applied to it.\n",
    "    In the final step the image will be converted to a Numpy Array using tf.cast\n",
    "    \"\"\"\n",
    "    # read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # decode the image\n",
    "    decoded_image = tf.image.decode_jpeg(contents=image, channels=1)\n",
    "    # convert image data type to float32\n",
    "    convert_imgs = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "    # resize and transpose \n",
    "    resized_image = tf.image.resize(images=convert_imgs, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.transpose(resized_image, perm = [1, 0, 2])\n",
    "\n",
    "    # to numpy array (Tensor)\n",
    "    image_array = tf.cast(image, dtype=tf.float32)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    This function gets the image path and \n",
    "    reads the image usin TensorFlow, Then the image will be decoded and \n",
    "    will be converted to float data type. next resize and transpose will be applied to it.\n",
    "    In the final step the image will be converted to a Numpy Array using tf.cast\n",
    "    \"\"\"\n",
    "    # read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # decode the image\n",
    "    decoded_image = tf.image.decode_jpeg(contents=image, channels=1)\n",
    "    # convert image data type to float32\n",
    "    convert_imgs = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "    # resize and transpose \n",
    "    resized_image = tf.image.resize(images=convert_imgs, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.transpose(resized_image, perm = [1, 0, 2])\n",
    "\n",
    "    # to numpy array (Tensor)\n",
    "    image_array = tf.cast(image, dtype=tf.float32)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(image_path, label:str):\n",
    "    \n",
    "    '''\n",
    "    The function takes an image path and label as input and returns a dictionary containing the processed image tensor and the label tensor. \n",
    "    First, it loads the image using the load_image function, which decodes and resizes the image to a specific size. Then it converts the given\n",
    "    label string into a sequence of Unicode characters using the unicode_split function. Next, it uses the char_to_num layer to convert each\n",
    "    character in the label to a numerical representation. It pads the numerical representation with a special class (n_classes)\n",
    "    to ensure that all labels have the same length (MAX_LABEL_LENGTH). Finally, it returns a dictionary containing the processed image tensor\n",
    "    and the label tensor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get the image\n",
    "    image = load_image(image_path)\n",
    "    # Convert the label into characters\n",
    "    chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "    # Convert the characters into vectors\n",
    "    vecs = char_to_num(chars)\n",
    "    \n",
    "    # Pad label\n",
    "    pad_size = MAX_LABEL_LENGTH - tf.shape(vecs)[0]\n",
    "    vecs = tf.pad(vecs, paddings = [[0, pad_size]], constant_values=n_classes+1)\n",
    "    \n",
    "    return {'image':image, 'label':vecs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(train_csv['FILENAME'].to_list()), np.array(train_csv['IDENTITY'].to_list()))\n",
    ").shuffle(1000).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Validation data\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(valid_csv['FILENAME'].to_list()), np.array(valid_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Testing data.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(test_csv['FILENAME'].to_list()), np.array(test_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size   : 24000\n",
      "Validation Data Size : 12000\n",
      "Testing Data Size    : 2400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size   : {tf.data.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Validation Data Size : {tf.data.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Testing Data Size    : {tf.data.Dataset.cardinality(test_ds).numpy() * BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(data, GRID=[4,4], FIGSIZE=(25, 8), cmap='binary_r', model=None, decode_pred=None):\n",
    "    \n",
    "    # Plotting configurations\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    n_rows, n_cols = GRID\n",
    "    \n",
    "    # Loading Data \n",
    "    data = next(iter(data))\n",
    "    images, labels = data['image'], data['label']\n",
    "    \n",
    "    # Iterate over the data \n",
    "    for index, (image, label) in enumerate(zip(images, labels)):\n",
    "        \n",
    "        # Label processing\n",
    "        text_label = num_to_char(label)\n",
    "        text_label = tf.strings.reduce_join(text_label).numpy().decode('UTF-8')\n",
    "        text_label = text_label.replace(\"[UNK]\", \" \").strip()\n",
    "        \n",
    "        # Create a sub plot\n",
    "        plt.subplot(n_rows, n_cols, index+1)\n",
    "        plt.imshow(tf.transpose(image, perm=[1,0,2]), cmap=cmap)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if model is not None and decode_pred is not None:\n",
    "            # Make prediction\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "            pred = decode_pred(pred)[0]\n",
    "            title = f\"True : {text_label}\\nPred : {pred}\"\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            # add title\n",
    "            plt.title(text_label)\n",
    "\n",
    "    # Show the final plot\n",
    "    cls()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 09:10:04.281200: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0871.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.281272: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0549.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.281700: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0602.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.281927: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0762.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282022: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0546.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282294: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0864.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282343: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0947.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282403: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0831.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282669: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0205.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282747: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0518.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282789: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0348.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.282868: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0450.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283008: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0618.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283074: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0070.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283131: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0403.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283174: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0080.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283208: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0584.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283287: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0484.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283436: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0768.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283592: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0818.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283706: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0366.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283760: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0800.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283821: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0576.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.283988: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0876.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284207: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0711.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284364: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0352.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284383: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0817.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284604: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0121.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284642: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0209.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284887: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0224.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284915: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0488.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284964: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0839.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.284993: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0127.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285147: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0124.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285278: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0586.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285372: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0492.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285386: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0907.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285425: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0573.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285485: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0394.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285547: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0478.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285605: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0955.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285829: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0430.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285877: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0884.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285921: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0977.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.285976: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0974.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286018: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0685.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286071: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0161.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286216: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0306.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286380: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0740.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286503: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0643.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286623: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0397.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.286660: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0191.jpg; No such file or directory\n",
      "2024-07-02 09:10:04.290566: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_1028.jpg; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/flasky/data/test_v2/TEST_0871.jpg; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mshow_images\u001b[0;34m(data, GRID, FIGSIZE, cmap, model, decode_pred)\u001b[0m\n\u001b[1;32m      5\u001b[0m n_rows, n_cols \u001b[38;5;241m=\u001b[39m GRID\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Loading Data \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Iterate over the data \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5887\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5888\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/flasky/data/test_v2/TEST_0871.jpg; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2500x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(data=train_ds, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(Layer):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # define the loss function \n",
    "        self.loss_function = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_hat):\n",
    "        # Get the batch length \n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "\n",
    "        # get the input and label lengths\n",
    "        input_len = tf.cast(tf.shape(y_hat)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_len = tf.cast(tf.shape(y_true)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_function(y_true, y_hat, input_len, label_len) \n",
    "\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "input_images = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), name=\"image\")\n",
    "\n",
    "# Labels : These are added for the training purpose.\n",
    "input_labels = Input(shape=(None, ), name=\"label\")\n",
    "\n",
    "### Convolutional layers\n",
    "# layer 1 \n",
    "conv_1 = Conv2D(64, 3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\", name=\"conv_1\")(input_images)\n",
    "# layer 2\n",
    "conv_2 = Conv2D(32, 3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\", name=\"conv_2\")(conv_1)\n",
    "max_pool_1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv_2)\n",
    "# layer 3\n",
    "conv_3 = Conv2D(64, 3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal', name=\"conv_3\")(max_pool_1)\n",
    "conv_4 = Conv2D(32, 3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal', name=\"conv_4\")(conv_3)\n",
    "max_pool_2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv_4)\n",
    "\n",
    "\n",
    "\n",
    "### Encoding \n",
    "reshape = Reshape(target_shape=((IMG_WIDTH//4), (IMG_HEIGHT//4)*32), name=\"reshape_layer\")(max_pool_2)\n",
    "dense_encoding = Dense(64, kernel_initializer=\"he_normal\", activation=\"relu\", name=\"enconding_dense\")(reshape)\n",
    "dense_encoding_2 = Dense(64, kernel_initializer=\"he_normal\", activation=\"relu\", name=\"enconding_dense_2\")(dense_encoding)\n",
    "dropout = Dropout(0.4)(dense_encoding_2)\n",
    "\n",
    "# Decoder\n",
    "lstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25), name=\"bidirectional_lstm_1\")(dropout)\n",
    "lstm_2 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.25), name=\"bidirectional_lstm_2\")(lstm_1)\n",
    "\n",
    "# Final Output layer\n",
    "output = Dense(len(char_to_num.get_vocabulary())+1, activation=\"softmax\", name=\"output_dense\")(lstm_2)\n",
    "\n",
    "# Add the CTC loss \n",
    "ctc_loss_layer = CTCLayer()(input_labels, output) \n",
    "\n",
    "# Define the final model\n",
    "model = Model(inputs=[input_images, input_labels], outputs=[ctc_loss_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_prediction(pred_label):\n",
    "    \"\"\"\n",
    "    This function has the job to decode the prediction that the model had.\n",
    "    The model predicts each character and then this function makes it readable. \n",
    "    \"\"\"\n",
    "    # Input length\n",
    "    input_len = np.ones(shape=pred_label.shape[0]) * pred_label.shape[1]\n",
    "    \n",
    "    # CTC decode\n",
    "    decode = tf.keras.backend.ctc_decode(pred_label, input_length=input_len, greedy=True)[0][0][:,:MAX_LABEL_LENGTH]\n",
    "    \n",
    "    # Converting numerics back to their character values\n",
    "    chars = num_to_char(decode)\n",
    "    \n",
    "    # Join all the characters\n",
    "    texts = [tf.strings.reduce_join(inputs=char).numpy().decode('UTF-8') for char in chars]\n",
    "    \n",
    "    # Remove the unknown token\n",
    "    filtered_texts = [text.replace('[UNK]', \" \").strip() for text in texts]\n",
    "    \n",
    "    return filtered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_sample_prediction(model, path):\n",
    "    \"\"\"\n",
    "    This function gets an image path and the model,\n",
    "    Loads and preprocesses the image and make predictions on it. \n",
    "    \"\"\"\n",
    "    # load image \n",
    "    image_loading = tf.io.read_file(path)\n",
    "    # decode image \n",
    "    decoded_image = tf.image.decode_jpeg(contents=image_loading, channels=1)\n",
    "    # convert the image data type to float \n",
    "    convert_image = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "    # resize the image \n",
    "    resized_image = tf.image.resize(images=convert_image, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    resized_image = tf.transpose(resized_image, perm = [1, 0, 2])\n",
    "    # convert image to array\n",
    "    image_array = tf.cast(resized_image, dtype=tf.float32)\n",
    "    # reshape image \n",
    "    single_image_data_with_batch = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # make prediction \n",
    "    prediction = decoder_prediction(model.predict(single_image_data_with_batch))\n",
    "\n",
    "    return prediction \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 08:12:02.193589: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0200.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.193632: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0709.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.193658: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0151.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.193701: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0672.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.193722: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0049.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.193817: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0865.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.194021: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0270.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.194600: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0160.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.194657: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0576.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195011: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0022.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195042: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0215.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195061: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0854.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195116: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0698.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195447: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0177.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195490: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0833.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.195567: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0608.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.196028: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0584.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.196168: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0551.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.196504: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0494.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.196516: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0038.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.196928: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0323.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197001: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0725.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197023: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0560.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197092: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0729.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197423: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0407.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197492: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0928.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197577: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0962.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.197768: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0704.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.198579: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0357.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.198707: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0406.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.198845: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0015.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.199346: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0803.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.199792: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0153.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.199886: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0756.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.199986: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0793.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200123: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_1011.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200330: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0937.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200353: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0438.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200484: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0373.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200670: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0393.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.200749: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0896.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.202731: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0886.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.203009: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0627.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.203188: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0989.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.203204: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0982.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.203385: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n/workspace/flasky/data/test_v2/TEST_0672.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_59643]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCALLBACKS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n/workspace/flasky/data/test_v2/TEST_0672.jpg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_59643]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flasky/data/test_v2/TEST_0563.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.203429: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0602.jpg; No such file or directory\n",
      "2024-07-02 08:12:02.204191: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /workspace/flasky/data/test_v2/TEST_0491.jpg; No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS, callbacks=CALLBACKS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
